{
    "description": "Signavio Data Ingestion",
    "component": "com.sap.system.python3Operator",
    "versionStatus": "active",
    "inports": [
        {
            "name": "csvInput",
            "type": "string"
        }
    ],
    "outports": [
        {
            "name": "print",
            "type": "string"
        },
        {
            "name": "stopping",
            "type": "string"
        }
    ],
    "icon": "puzzle-piece",
    "iconsrc": "dispatch.svg",
    "config": {
        "$type": "http://sap.com/vflow/signavio_data_ingestion.configSchema.json",
        "custom_schema": false,
        "from_qualtrics": true,
        "script": "import pandas as pd\nimport numpy as np\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\nimport json\nimport datetime\nfrom io import StringIO\n\n\ndef return_avro(d, datetime_format):\n    return int(datetime.datetime.timestamp(datetime.datetime.strptime(str(d), datetime_format)))\n\nclass SignavioIngestion:\n    def __init__(self, file):\n        self.file = pd.read_csv(StringIO(file), sep=\",\")\n        \n        #table_name\n        self.table_name = api.config.table_name\n        api.send(\"print\", str(self.table_name))\n        \n        #signavio_connection\n        self.signavio = api.config.signavio_connection[\"connectionProperties\"]\n        self.token = self.signavio[\"password\"]\n        #api.send(\"print\", str(self.token))\n        self.url = str(\"https://\" + self.signavio[\"host\"] + \"/ingestion/data\")\n        #api.send(\"print\", str(self.url))\n        self.headers = {\"Authorization\": \"Bearer \" + str(self.token)}\n        #api.send(\"print\", str(self.headers))\n        \n        #primary_key        \n        try:\n            self.primary_key = api.config.primary_key\n        except:\n            self.primary_key = self.file.columns[0]\n        api.send(\"print\", \"Primary Key: \" + str(self.primary_key))\n\n        #custom_schema\n        self.custom_schema = api.config.custom_schema\n        api.send(\"print\", \"Custom Schema: \" + str(self.custom_schema))\n\n        #data_schema / columns\n        try:\n            temp = api.config.data_schema.replace(\"'\", '\"')\n            self.columns = json.loads(temp)\n        except:\n            self.columns = None\n        # handle illegal characters (\".\", what others?)\n        list(map(lambda x: self.file.rename(columns={x: x.replace('.','')}, inplace=True), self.file.columns.values))\n        list(map(lambda x: self.file.rename(columns={x: x.replace(' ','')}, inplace=True), self.file.columns.values))\n        list(map(lambda x: self.file.rename(columns={x: x.replace('/','')}, inplace=True), self.file.columns.values))\n        # add other illegal characters handling\n        api.send(\"print\", \"Columns in Schema: \" + str(self.columns))\n\n        #from_qualtrics\n        try:\n            self.from_qualtrics = api.config.from_qualtrics\n        except:\n            self.from_qualtrics = False\n        #api.send(\"print\", str(self.from_qualtrics))\n        \n        #datetime_columns\n        try:\n            # TODO: reformat input into list of strings\n            self.datetime_columns = api.config.datetime_columns\n        except:\n            if self.from_qualtrics is True:\n                self.datetime_columns = [\"StartDate\", \"EndDate\"]\n            else:\n                self.datetime_columns = []\n        #api.send(\"print\", str(\"THIS IS IMPORTANT\"))\n        #api.send(\"print\", str(self.datetime_columns))\n        \n        #datetime_format\n        try:\n            self.datetime_format = api.config.datetime_format\n        except:\n            if self.from_qualtrics is True:\n                self.datetime_format = \"%Y-%m-%d %H:%M:%S\"\n            else:\n                self.datetime_format = None\n        #api.send(\"print\", str(self.datetime_format))\n\n        self.b_file = None\n        self.data = None\n        pass\n    \n\n    def prepare_file(self):\n        \n        if self.from_qualtrics:\n            if \"ResponseID\" in self.file.columns:\n                self.file.rename(columns={\"ResponseID\": \"ResponseId\"}, inplace=True)\n                self.primary_key = \"ResponseId\"\n            if \"ExternalDataReference\" in self.file.columns:\n                self.file.rename(columns={\"ExternalDataReference\": \"ExternalReference\"}, inplace=True)\n            api.send(\"print\", \"Columns extracted from source object: \" + str(self.file.columns))\n        \n        if self.datetime_format:\n            for column in self.datetime_columns:\n                if column in self.file.columns:\n                    self.file[column] = self.file[column].apply(lambda x: return_avro(x, self.datetime_format))\n\n        temp_file = str(\"temp_\" + self.table_name + \".csv\")\n        #api.send(\"print\", str(temp_file))\n        self.file.to_csv(temp_file, index=False)\n        self.b_file = open(temp_file, \"rb\")\n        api.send(\"print\", \"File Data: \" + str(self.file))\n    \n    \n    def prepare_columns(self):\n        columns = []\n        mapping = {\n            np.dtype('int64'): \"long\",\n            np.dtype('float64'): \"double\",\n            np.dtype('object'): \"string\",\n            np.dtype('bool'): \"boolean\"\n        }\n        for c in self.file.columns:\n            if c in self.datetime_columns:\n                columns.append({\"name\": c, \"type\": {\n                    \"type\": \"long\",\n                    \"logicalType\": \"timestamp-millis\"\n                }})\n            else:\n                try:\n                    columns.append({\"name\": c, \"type\": [\"null\", mapping[self.file[c].dtype]]})\n                except KeyError:\n                    columns.append({\"name\": c, \"type\": [\"null\", \"string\"]})\n        #api.send(\"print\", str(columns))\n        return columns\n        \n        \n    def prepare_schema(self):\n        if self.columns is None:\n            self.columns = self.prepare_columns()\n            api.send(\"print\", str(self.columns))\n        schema = {\"type\": \"record\", \"name\": self.table_name, \"fields\": self.columns}\n        self.data = {\"schema\": json.dumps(schema), \"primaryKeys\": self.primary_key}\n\n\n    def post(self):\n        response = requests.post(\n            self.url,\n            headers=self.headers,\n            data=self.data,\n            files={\"file1\": self.b_file}\n        )\n        api.send(\"print\", str(response))\n        api.send(\"print\", str(response.text))\n        api.send(\"print\", \"Data: \" + str(self.data))\n\n\ndef ingestion_main(file):\n    signavio_ingestion = SignavioIngestion(file)\n    signavio_ingestion.prepare_file()\n    signavio_ingestion.prepare_schema()\n    signavio_ingestion.post()\n    api.send(\"stopping\", \"stopping\")\n    pass\n\nif __name__ == \"__main__\":\n    api.set_port_callback(\"csvInput\", ingestion_main)\n    pass\n\n"
    },
    "tags": {
        "aiohttp": ""
    }
}